{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zillow Regression Analysis\n",
    "**Artifact: Jupyter Notebook Report**\n",
    "\n",
    "Created by: Mijail Q. Mariano\n",
    "\n",
    "Wednesday, July 27th 2022\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook dependencies\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# key libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from math import sqrt\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib as mlp\n",
    "mlp.rcParams['figure.dpi'] = 300\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# created zillow .py files and functions\n",
    "from prepare import display_all\n",
    "import acquire\n",
    "from acquire import get_zillow_dataset, \\\n",
    "            clean_zillow_dataset, \\\n",
    "            zillow_outliers, \\\n",
    "            plot_target, \\\n",
    "            plot_continuous, \\\n",
    "            plot_discrete, \\\n",
    "            train_validate_test_split, \\\n",
    "            select_kbest, \\\n",
    "            recursive_feature_eng\n",
    "\n",
    "\n",
    "# sklearn library for modeling\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, RFE, RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### <u>**``Focal Analysis Questions:``**</u>\n",
    "\n",
    "**``Target Variable:``**\n",
    "* Tax_Value_Dollar_Count = **\"home_value\"**\n",
    "\n",
    "\n",
    "**1. Does a home's location matter?**\n",
    "\n",
    "I presume location matters in determining the final value/price of a home, as there could be economic factors (e.g., cost-of-living/inflationary considerations) that may be considered when assessing a home's relative value. Additionally, homes typically closer to metropolitan or labor producing areas may be more costly than rural/less-labor promoted areas.\n",
    "\n",
    "* Homes closer to schools (predicted positive relationship)\n",
    "* Home closer to parks/recreational areas (predicted positive relationship)\n",
    "* Homes near hospitals/hospice care (predicted positive relationship)\n",
    "* Areas with relatively less crime rates or high law-enforcement presence. (predicted negative relationship)\n",
    "* Given the additional insurance costs/natural disaster considerations homes near bodies of water may be less attractive to home buyers. (predicted negative relationship)\n",
    "\n",
    "``Relative Features:``\n",
    "1. Fips\n",
    "2. Latitude \n",
    "3. Longitude \n",
    "4. Parcel_id\n",
    "5. Region_City_id\n",
    "6. Region_id_County\n",
    "7. Region_id_Zip\n",
    "\n",
    "\n",
    "**2. Does home space matter?**\n",
    "\n",
    "I presume that the larger the home (as measured by sq. ft.), the higher the home value will be. \n",
    "\n",
    "A larger home can be more attractive to home buyers since the space can act as both an initial family home, but also their “forever home”. Meaning that people who might not initially have a use for the additional space - may see the potential benefits of having it when they are ready to either 1. expand their family/or find use for the space or 2. View the space as a future investment for a buyer willing to pay the same or more for their spacious home in the future (investment thinking). \n",
    "\n",
    "* Consider the total number of baths \n",
    "* Consider the total number of bedrooms\n",
    "\n",
    "``Relative Features:``\n",
    "1. Calculated_Finished_Sq_Feet\n",
    "2. Bathroom_Count\n",
    "3. Bedroom_Count\n",
    "4. Full_Bath_Count\n",
    "\n",
    "\n",
    "**3. Does the home selling or purchase period matter?**\n",
    "\n",
    "I presume that the period when a home is purchased or placed on the market will matter. Home buyers may be more reluctant to purchase a home in the colder regional months (e.g., typically winter) when the weather may be less favorable for moving. \n",
    "\n",
    "There may also be *renter factors or periods in the year when leases end and renters make the decision to purchase a home, subsequently driving more buyers to the market and thus potentially increasing home sale values. \n",
    "\n",
    "*(higher demand + “same” or not enough supply = more competition/higher home purchase price)*\n",
    "\n",
    "* Consider seasonal patterns (e.g., summer months vs. winter) \n",
    "\n",
    "``Relative Features:``\n",
    "1. Transaction_Date\n",
    "2. Year_built\n",
    "3. Assessment_Year\n",
    "\n",
    "\n",
    "**<u>Features not taken forward after initial query pull:</u>**\n",
    "\n",
    "- **parcel_id** \n",
    "  - Though may be needed in final predictions/report\n",
    "<br> </br>\n",
    "\n",
    "- **room_count** \n",
    "  - This is the total number of rooms in a home. Omitting this since it may be too closely associated to other features such as bathrooms and bedrooms. Additionally, this analysis presumes that all homes have standard rooms such as kitchens and living-rooms.\n",
    "<br> </br>\n",
    "- calculated bath and bedroom\n",
    "- full bath count\n",
    "- census tract and block\n",
    "- finished squared feet 12\n",
    "- property county land use code\n",
    "- raw census tract and block\n",
    "- assessment tax year\n",
    "- land tax value dollar count\n",
    "- structure tax value dollar count\n",
    "- tax amount\n",
    "- structure tax value dollar count\n",
    "- assessment tax year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Data Acquisition and Preparation``**\n",
    "\n",
    "**``Key Highlights``**\n",
    "\n",
    "- Used domain knowledge and research to focus on key questions for analysis\n",
    "- Renamed and converted columns/features to proper type\n",
    "- Dropped initial features with > 20% Null values\n",
    "- Labeled and omitted homes with recorded finished sq. footage > 8K as larger/outlier homes*\n",
    "- Focused on home values within the 90th percentile ~900K\n",
    "\n",
    "*these homes represented homes larger than the majority homes in the dataset, therefore making the analysis or future prediction less accurate or obscured when including these records in the analysis.* *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquiring and preparing initial zillow dataset\n",
    "\n",
    "df = get_zillow_dataset()\n",
    "df = clean_zillow_dataset(df)\n",
    "df = zillow_outliers(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the data types\n",
    "pd.DataFrame(df.dtypes.sort_values()).rename(columns = {0: \"data_type\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics for continuous variables/features\n",
    "summary_stats = df.describe().T\n",
    "summary_stats[\"range\"] = summary_stats[\"max\"] - summary_stats[\"min\"]\n",
    "summary_stats.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**``Initial Univariate Feature Analysis:``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring/visualizing the target - tax dollar home value (\"home_value\")\n",
    "\n",
    "plt.figure(figsize = (14, 8))\n",
    "sns.set(font_scale = 1)\n",
    "ax = sns.histplot(df[\"home_value\"], kde = True)\n",
    "\n",
    "# removing axes scientific notation \n",
    "ax.ticklabel_format(style = \"plain\")\n",
    "\n",
    "plt.axvline(df[\"home_value\"].mean(), linewidth = 2, color = 'purple', alpha = 0.5, label = \"mean\")\n",
    "plt.text(335500, 1400, \"$333,952\", horizontalalignment='left', size='medium', color='purple', weight='semibold')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Home Value\")\n",
    "plt.legend()\n",
    "plt.title(\"Home Value Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# home values box plot\n",
    "plt.figure(figsize = (10, 6))\n",
    "plt.boxplot(df[\"home_value\"])\n",
    "\n",
    "ax.set(xlabel = None)\n",
    "plt.title(\"Home Values: Remaining Outliers\")\n",
    "plt.xlabel(None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total makeup of homes by value bin\n",
    "plot_target(df[\"home_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting continuous/numerical features\n",
    "plot_continuous(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting discreate/categorical features\n",
    "plot_discrete(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>``Note on 'county_id':``</u>\n",
    "\n",
    "* the sns.countplot for 'county_id' shows a 1for1 frequency match with fips_code\n",
    "* omitting county_id as this is may be too closely associated with fips_code and is currently more ambigous in exact location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new feature/variable for month transaction/purchases in 2017\n",
    "df = acquire.get_months_and_plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <u>``Pre-processing the Data:``</u>\n",
    "\n",
    "**``Highlights``**\n",
    "\n",
    "- Established individual feature class (continuous/discrete)\n",
    "- Generated new features potentially more descriptive of the data\n",
    "- Created dummy columns/variables for discrete features\n",
    "- Dropped redundant columns/features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_lst = []\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() > 20:\n",
    "        output = {\n",
    "            'Feature': col,\n",
    "            'Unique Values': df[col].nunique(),\n",
    "            'Feature Class': \"continuous\"}\n",
    "    else:\n",
    "        output = {\n",
    "            'Feature': col,\n",
    "            'Unique Values': df[col].nunique(),\n",
    "            'Feature Class': \"discrete\"}\n",
    "    \n",
    "    unique_lst.append(output)\n",
    "\n",
    "pd.DataFrame(unique_lst).sort_values('Unique Values', ascending = False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column to calculate total number of elapsed years through current year\n",
    "df = acquire.age_of_homes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column to clean up month-year column\n",
    "df = acquire.clean_months(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new columns for binning bathrooms and bedrooms\n",
    "df = acquire.bin_bath_and_beds(df)\n",
    "df.shape\n",
    "# df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new column for half baths\n",
    "df = acquire.get_half_baths(df)\n",
    "print(f'dataframe shape: {df.shape}')\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Dummy Columns/Variables for Discrete/Categorical Features\n",
    "dummy_df = pd.get_dummies(data = df, columns = ['fips_code', 'purchase_month'])\n",
    "df = dummy_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping redundant columns & 'county_id' column\n",
    "df = acquire.drop_after_dummy(df)\n",
    "print(f'New Dataframe W/Dummy Variables: {df.shape}')\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### **Splitting Zillow Dataset for Exploration/Hypothesis Testing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validate, test = train_validate_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing a home value baseline prediction\n",
    "train, validate = acquire.establish_baseline(train, validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset subsplits (x and y variables)\n",
    "X_train = train.drop(columns = \"home_value\")\n",
    "y_train = train['home_value']\n",
    "\n",
    "X_validate = validate.drop(columns = \"home_value\")\n",
    "y_validate = validate['home_value']\n",
    "\n",
    "X_test = test.drop(columns = \"home_value\")\n",
    "y_test = test['home_value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Exploration and Hypothesis Testing``**\n",
    "\n",
    "**<u>``Two Sample T-test Hypothesis:``</u>**\n",
    "* $H_0$ Null Hypothesis: The home value average for all feature values are the same\n",
    "* $H_a$ Alternative Hypothesis: The home value average across feature values are different\n",
    "\n",
    "**<u>``Pearson R Correlation Hypothesis:``</u>**\n",
    "* $H_0$ Null Hypothesis: There is no linear correlation between the variables\n",
    "* $H_a$ Alternative Hypothesis:  There is a linear correlation between variables\n",
    "\n",
    "$alpha$: 0.05\n",
    "\n",
    "<u>**Definitions:**</u>\n",
    "\n",
    "The *\"t_score\"* is a ratio between the difference of two groups and the difference within the groups:\n",
    "- Larger t_score = more difference between groups\n",
    "- Smaller t_score = more similarity between groups\n",
    "\n",
    "The *\"p_value\"* is the evidence against a null hypothesis: \n",
    "\n",
    "- Smaller p_value = the stronger the evidence that you should reject the null hypothesis\n",
    "\n",
    "\n",
    "|feature|class|test|\n",
    "|----|----|----|\n",
    "|'bathroom-binned'|discrete|ind. t-test|\n",
    "|'bedroom-binned'|discrete|ind. t-test|\n",
    "|'fips_code'|discrete|ind. t-test|\n",
    "|'purchase_month|discrete|ind. t-test|\n",
    "|'living_sq_feet'|continuous|pearson-r|\n",
    "|'latitude'|continuous|pearson-r|\n",
    "|'longitude'|continuous|pearson-r|\n",
    "|'property_sq_feet'|continuous|pearson-r|\n",
    "|'city_id'|continuous|pearson-r|\n",
    "|'zip_code'|continuous|pearson-r|\n",
    "|'year_built'|continuous|pearson-r|\n",
    "|'home_age'|continuous|pearson-r|\n",
    "\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting continuous variables against target\n",
    "cont_lst = train.select_dtypes(exclude = [\"bool\", \"uint8\", \"object\"]).columns.tolist()\n",
    "cont_df = train[cont_lst]\n",
    "acquire.plot_variable_pairs1(cont_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting non-dummy variables against target\n",
    "pair_lst = train.select_dtypes(exclude = [\"bool\", \"uint8\"]).columns.tolist()\n",
    "acquire.plot_variable_pairs2(train, pair_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_lst = train.select_dtypes(include = [\"bool\", \"uint8\"]).columns.tolist()\n",
    "cont_lst = train.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()\n",
    "\n",
    "target_mean = round(y_train.mean(), 2)\n",
    "alpha = 0.05\n",
    "\n",
    "metrics = []\n",
    "for col in cat_lst:\n",
    "    sub_group = train[train[col] == 1].home_value\n",
    "    t_score, p_value = stats.ttest_1samp(sub_group, target_mean)\n",
    "\n",
    "    if p_value < alpha:\n",
    "        output = {\n",
    "            \"discrete feature\": col,\n",
    "            \"t_score\": t_score,\n",
    "            \"p_value\": p_value}\n",
    "        \n",
    "        metrics.append(output)\n",
    "\n",
    "    else:\n",
    "        print(f'Column: {col} not statistically significant.')\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "\n",
    "categorical_scores = pd.DataFrame(metrics)\n",
    "categorical_scores.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train[\"home_value\"]\n",
    "alpha = 0.05\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for col in cont_lst:\n",
    "    r, p_value = stats.stats.pearsonr(train[col], target)\n",
    "\n",
    "    if p_value < alpha:\n",
    "        output = {\n",
    "            \"continuous feature\": col,\n",
    "            \"correlation coeffficient\": r,\n",
    "            \"p_value\": p_value}\n",
    "        \n",
    "        metrics.append(output)\n",
    "\n",
    "    else:\n",
    "        print(f'Column: {col}')\n",
    "        print(f'P_value: {round(p_value, 4)}')\n",
    "        print('Not statistically significant.')\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "\n",
    "continuous_scores = pd.DataFrame(metrics)\n",
    "continuous_scores.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**``Statiscal Analysis/Conclusion Notes:``**\n",
    "\n",
    "* Given the low/no statistical significance found in the following features or values, I elect to drop or disregard the following feature options in the datasets used in modeling. \n",
    "* Looking ahead, I will want to further examing/explore why these feature or options are considered \"statistically insignificant\" when determining \"home_value\".\n",
    "\n",
    "<u>``Continuous features:``</u>\n",
    "1. \"city_id\"\n",
    "2. \"property_sq_feet\"\n",
    "3. \"longitude\"\n",
    "4. \"zip_code\"\n",
    "\n",
    "<u>``Categorical/discrete values:``</u>\n",
    "1. '4_to_6.5_baths'\n",
    "2. '3_to_4_bedrooms'\n",
    "3. '5_to_6_bedrooms'\n",
    "4. 'purchase_month_February'\n",
    "5. 'purchase_month_June'\n",
    "6. 'county_id'*\n",
    "\n",
    "county_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_validate shape: {X_validate.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning independent feature datasets -- preparing for modeling:\n",
    "X_train = acquire.clean_for_features(X_train)\n",
    "X_validate = acquire.clean_for_features(X_validate)\n",
    "X_test = acquire.clean_for_features(X_test)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_validate shape: {X_validate.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **``Modeling``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of continuous variables scaled: using SKlearn Robust Scaler\n",
    "cont_lst = X_train.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()\n",
    "\n",
    "for col in cont_lst:\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(X_train[[col]])\n",
    "\n",
    "    x_scaled = scaler.transform(X_train[[col]])\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(121)\n",
    "    ax = sns.histplot(X_train[[col]], bins = 25, edgecolor = 'black', label = col)\n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Original: {col}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax = sns.histplot(x_scaled, bins=25, edgecolor = 'black', label = \"scaled\")\n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Scaled: {col}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling necessary features\n",
    "X_train = acquire.scaled_data(X_train)\n",
    "X_validate = acquire.scaled_data(X_validate)\n",
    "X_test = acquire.scaled_data(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the scaling\n",
    "X_train.home_age.describe() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running K_best function for top 10 independent features in training dataset\n",
    "pd.DataFrame(acquire.select_kbest(X_train, y_train, 10)).rename(columns = {0: \"Feature\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Sklearn Recursive Function for top 10 grouped functions in dataset\n",
    "acquire.recursive_feature_eng(X_train, y_train, 10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``A Note on Sklearn's Feature Engineering \"RFECV\" Function``**\n",
    "\n",
    "<u>**Cross-validation Estimator:**</u>\n",
    "\n",
    "* An estimator that has built-in cross-validation capabilities to automatically select the best hyper-parameters. \n",
    "* Advantages of using a cross-validation estimator is that they can take advantage of warm-starting by reusing precomputed results in the previous steps of the cross-validation process. This generally leads to speed improvements. \n",
    "* By default, all these estimators, apart from RidgeCV with an LOO-CV, will be refitted on the full training dataset after finding the best combination of hyper-parameters.\n",
    "\n",
    "<u>**Scorer:**</u>\n",
    "\n",
    "* A non-estimator callable object which evaluates an estimator on given test data, returning a number. Unlike evaluation metrics, a greater returned number must correspond with a better score. See The scoring parameter: defining model evaluation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating and fitting Sklearn's RFECV feature selection function\n",
    "RFECV = RFECV(\n",
    "    estimator = LinearRegression(),\n",
    "    min_features_to_select = 5)\n",
    "\n",
    "RFECV = RFECV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected number of features\n",
    "feature_lst = X_train.columns[RFECV.support_].tolist()\n",
    "pd.DataFrame(feature_lst).rename(columns = {0: \"Features\"}).sort_values(\"Features\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``Generating Models & Scoring against Train Dataset: Linear Regression, Lasso Lars, and Tweedie Regressor``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr_model = lr.fit(X_train, y_train)\n",
    "\n",
    "lars = LassoLars()\n",
    "lars_model = lars.fit(X_train, y_train)\n",
    "\n",
    "glm = TweedieRegressor(alpha = 1, power = 0)\n",
    "glm_model = glm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training R-squared w/Linear Regression:\", lr_model.score(X_train, y_train).round(4))\n",
    "print(\"Training R-squared w/Lasso Lars:\", lars_model.score(X_train, y_train).round(4))\n",
    "print(\"Training R-squared w/Tweedie Regressor:\", glm_model.score(X_train, y_train).round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**``Deploying Models on Validate Dataset: Mean Squared Error (MSE)``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [lr_model, lars_model, glm_model]\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    train_model = model.predict(X_train)\n",
    "    rmse_train = sqrt(mean_squared_error(y_train,\n",
    "                                         train_model))\n",
    "    \n",
    "    validate_model = model.predict(X_validate)\n",
    "    rmse_validate = sqrt(mean_squared_error(y_validate,\n",
    "                                         validate_model))\n",
    "    \n",
    "    print('RMSE for {} model on the train dataset: {}'.format(model, round(rmse_train, 2)))\n",
    "    print('RMSE for {} model on the validate dataset: {}'.format(model, round(rmse_validate, 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### <u>**``Model Plots and Evaluation on Validate Dataset:``**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the independent and dependent variables\n",
    "X_var = pd.DataFrame(X_validate[feature_lst])\n",
    "y_var = pd.DataFrame(y_validate)\n",
    "model_df = pd.concat([X_var, y_var], axis = 1).reset_index(drop = True)\n",
    "\n",
    "# creating a home value mean baseline prediction\n",
    "baseline_mean_predictions = round(y_validate.mean(), 2)\n",
    "model_df[\"baseline_mean_predictions\"] = baseline_mean_predictions\n",
    "\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating validate model predictions and assigning to dataframe\n",
    "lr_predictions = lr_model.predict(X_validate)\n",
    "model_df[\"linear_predictions\"] = lr_predictions.round(2)\n",
    "\n",
    "lars_predictions = lars_model.predict(X_validate)\n",
    "model_df[\"lars_predictions\"] = lars_predictions.round(2)\n",
    "\n",
    "glm_predictions = lars_model.predict(X_validate)\n",
    "model_df[\"glm_predictions\"] = glm_predictions.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating error reports for all models and baseline:\n",
    "SSE, ESS, TSS, MSE, RMSE = acquire.get_error_report(model_df[\"home_value\"], model_df[\"linear_predictions\"])\n",
    "print()\n",
    "SSE, ESS, TSS, MSE, RMSE = acquire.get_error_report(model_df[\"home_value\"], model_df[\"lars_predictions\"])\n",
    "print()\n",
    "SSE, ESS, TSS, MSE, RMSE = acquire.get_error_report(model_df[\"home_value\"], model_df[\"glm_predictions\"])\n",
    "print()\n",
    "SSE, ESS, TSS, MSE, RMSE = acquire.get_error_report(model_df[\"home_value\"], model_df[\"baseline_mean_predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquiring melted model column for easier plotting\n",
    "melt_df = acquire.get_melted_table(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model residuals\n",
    "acquire.plot_model_residuals(melt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting model predicted home values against target\n",
    "acquire.plot_models(melt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual Home Values and Model Predicted Values\n",
    "acquire.model_distributions(model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>``Deploying Linear Regression Model on Test Dataset:``</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a dataframe\n",
    "test_df = pd.DataFrame(y_test)\n",
    "\n",
    "# generating model predictions\n",
    "test_df[\"model_predictions\"] = lr_model.predict(X_test).round(2)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning R-squared score & RMSE on test dataset\n",
    "rmse_test = sqrt(mean_squared_error(test_df['home_value'], test_df['model_predictions']))\n",
    "print(\"Training R-squared w/Linear Model:\", lr_model.score(X_test, y_test).round(4))\n",
    "print(f'RMSE for OLS model on the test dataset: {round(rmse_test, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning RMSE report \n",
    "acquire.final_rmse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**<u>``Analysis Summary``</u>**\n",
    "\n",
    "Overall, the linear regression model performed at ~16% better accuracy than a baseline mean home value predictor. Though not entirely conclusive of a home's tax assessed value - I believe this model may be able to handle fluctuations in the overall market particularly well. \n",
    "\n",
    "By using \"binned\" or categorical features in traditionally sought after home characteristics (e.g., bedrooms, bathrooms) to determine a home's value, the model helps to handle external factors such as seasonality/seasonal effects, cultural preferences, or demand shocks that can undoubtedly impact the number of 'for sale' homes available and subsquently, house prices in relatively short periods.\n",
    "\n",
    "\n",
    "**``Recommendations:``**\n",
    "\n",
    "1. Create a **\"Real-estate Training Program** that aims at helping Real-estate Brokers/Agents, Marketing, and Real-estate Consultancy teams to familiarize themselves with seasonal patterns in their local areas. By offering this program to real-estate professionals who are often closest to both sellers and buyers, it would help them to:\n",
    "\n",
    " - Better advise their clients on the most optimal periods to enter the market (purchase/sell their home)\n",
    " - More quickly recognize housing market shocks and make real-time decisions that help to normalize home value prices \n",
    "\n",
    "2. Use our online, mobile application, and advisory platforms to promote renovations of not just older homes, but smaller spaces and potentially converting them to **half-baths**. This feature along with more finished living space appears to be appealing characteristics for home buyers trading in traditional features such as the number of bedrooms or bathrooms for more efficient and univarsal home space.\n",
    "\n",
    "----\n",
    "\n",
    "**Looking Ahead (next steps):**\n",
    "\n",
    "- improve the model's predictive accuracy by identifying, testing, and including other potential home market factors \n",
    "    - regional cost-of-living indices\n",
    "    - unemployment rates\n",
    "    - educational/school ratings\n",
    "    - crime rates\n",
    "    - home design styles\n",
    "<br>\n",
    "- calculate home/area distance to nearby metropolitan cities, park/recreational areas, schools, hospitals/hospice centers, etc.\n",
    "- parse out fips codes into more distinct locations either by towns/villages/neighborhoods or exact cities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
