{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# importing libraries/modules, and functions\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# visualization libraries\n",
    "import matplotlib as mlp\n",
    "mlp.rcParams['figure.dpi'] = 300\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# created zillow library and functions\n",
    "from acquire import get_zillow_dataset, clean_zillow_dataset, zillow_outliers, train_validate_test_split, select_kbest, recursive_feature_eng\n",
    "from prepare import display_all\n",
    "\n",
    "# sklearn library for data science\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, RFE, f_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### ``Data Acquisition & Preparation:``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial query for MySQL zillow data\n",
    "# query = '''\n",
    "# SELECT *\n",
    "#     FROM properties_2017\n",
    "#         JOIN predictions_2017 USING (id)\n",
    "#             JOIN propertylandusetype USING (propertylandusetypeid)\n",
    "#                 WHERE transactiondate = 2017\n",
    "#                     AND propertylandusedesc = \"Single Family Residential\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling the MySQL zillow data\n",
    "# url = get_connection(user, password, host, \"zillow\")\n",
    "# df = pd.read_sql(query, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a csv file for storing\n",
    "# df.to_csv(\"/Users/mijailmariano/codeup-data-science/regression-exercises/zillow_regression.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can now pull the zillow data using the following function:\n",
    "zillow_df = get_zillow_dataset()\n",
    "display_all(zillow_df.head()) # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the dataset further\n",
    "initial_shape = zillow_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that bypasses pd row/cols limits\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     display(zillow_df.isnull().sum())\n",
    "\n",
    "display_all(zillow_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking total null percentage for ea. column\n",
    "display_all(round(zillow_df.isnull().mean(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning df for for feature with NULL % > 20% \n",
    "# though this may not be a necessary step at this time\n",
    "# consider first, choosing features/variables from research/domain knowledge that may be closely tied to home value\n",
    "# from these features/variables turn them into questions/hypothesis for exploration\n",
    "\n",
    "dropped_cols = []\n",
    "for col in zillow_df.columns:\n",
    "    if zillow_df[col].isnull().mean() > 0.2:\n",
    "        dropped_cols.append(col)\n",
    "        zillow_df = zillow_df.drop(columns = col)\n",
    "\n",
    "# returning initial shape vs. null drop shape\n",
    "print(f'initial df shape: {initial_shape}')\n",
    "print(f'shape after null drop: {zillow_df.shape}')\n",
    "print('dropped columns:', *dropped_cols, sep = '\\n- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see all \"id\" columns that remain\n",
    "# i presume that i will not need most, if not all of them - but let's check anyways\n",
    "\n",
    "mask = zillow_df.columns.str.contains(\"id\")\n",
    "zillow_df.iloc[:, mask].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will drop column \"parcelid.1\" from the mysql predictions_2017 table as this does not appear to have any significance to my current zillow dataframe\n",
    "zillow_df[[\"parcelid\", \"parcelid.1\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_df = zillow_df.drop(columns = \"parcelid.1\")\n",
    "\n",
    "print(zillow_df.shape)\n",
    "zillow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remaining columns/features include:\n",
    "pd.Series(zillow_df.columns.sort_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <u>**``Initial Questions for Exploration:``**</u>\n",
    "\n",
    "**``Target Variable:``**\n",
    "* Tax_Value_Dollar_Count\n",
    "\n",
    "``Features That May* Lead to Data Leakage:``\n",
    "- land tax value dollar count\n",
    "- structure tax value dollar count\n",
    "- tax amount\n",
    "- land tax value dollar count\n",
    "- structure tax value dollar count\n",
    "\n",
    "\n",
    "**1. Does the home location matter? (must try) model per county**\n",
    "\n",
    "I presume location matters in determining the final value/price of a home, as there could be economic factors (e.g., cost-of-living/inflationary considerations). Additionally, homes typically closer to metropolitan or labor creating areas (employment) may be more costly to live-in than more rural/less-labor promoted areas.\n",
    "\n",
    "* Homes closer to schools (positive)\n",
    "* Home closer to parks/recreational areas (positive)\n",
    "* Homes near hospitals/hospice care (positive)\n",
    "* Areas with “less” crime rates or law-enforcement? (positive)\n",
    "* Areas near bodies of water may be less attractive? Given the additional insurance costs/natural disaster considerations (negative)\n",
    "\n",
    "``Features:``\n",
    "1. Fips\n",
    "2. Latitude \n",
    "3. Longitude \n",
    "4. Parcel_id\n",
    "5. Region_City_id\n",
    "6. Region_id_County\n",
    "7. Region_id_Zip\n",
    "\n",
    "\n",
    "**2. Does the size of the home matter?**\n",
    "\n",
    "I presume that the larger the home (as measured by sq. ft.), the higher the home value will be. A larger home can be more attractive to home buyers since the space can act as both an initial family home, but also a “forever home”. Meaning that people who might not initially have a use for the additional space - may see the potential benefits of having it when they are ready to either 1. Expand their family/or use of the space or 2. View the space as a future investment for someone willing to pay the same or more for this space (investment thinking). \n",
    "\n",
    "* Consider the total number of baths \n",
    "* Consider the total number of rooms\n",
    "\n",
    "``Features:``\n",
    "1. Bathroom_Count\n",
    "2. Bedroom_Count\n",
    "3. Calculated_Finished_Sq_Feet\n",
    "4. Full_Bath_Count\n",
    "\n",
    "**3. Does the period of when a home is purchased matter?**\n",
    "\n",
    "I presume that when* a home is placed on the market/is purchased matters. Home buyers may be more reluctant to purchase a home in the colder regional months (e.g., typically winter) when the weather may be less favorable for moving. There may also be *renter factors or periods in the year when leases end and renters make the decision to purchase a home, subsequently driving more buyers to the market and thus potentially increasing home values. \n",
    "\n",
    "*(higher demand + “same” or not enough supply = more competition/higher home purchase price)*\n",
    "\n",
    "* Consider seasonal patterns (e.g., summer months vs. winter) \n",
    "\n",
    "``Features:``\n",
    "1. Assessment_Year\n",
    "2. Year_built\n",
    "3. Transaction_Date\n",
    "\n",
    "\n",
    "**<u>Features not taken forward after initial query pull:</u>**\n",
    "\n",
    "- **parcel_id** \n",
    "  - Though may be needed in final predictions/report\n",
    "<br> </br>\n",
    "\n",
    "- **room_count** \n",
    "  - This is the total number of rooms in a home. Omitting this since it may be too closely associated to other features such as bathrooms and bedrooms. Additionally, this analysis presumes that all homes have standard rooms such as kitchens and living-rooms.\n",
    "<br> </br>\n",
    "- calculated bath and bedroom\n",
    "- full bath count\n",
    "- census tract and block\n",
    "- finished squared feet 12\n",
    "- property county land use code\n",
    "- raw census tract and block\n",
    "- assessment tax year\n",
    "- land tax value dollar count\n",
    "- structure tax value dollar count\n",
    "- tax amount\n",
    "- structure tax value dollar count\n",
    "- assessment tax year\n",
    "\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_cols = [\n",
    "'taxvaluedollarcnt',\n",
    "'bathroomcnt',\n",
    "'bedroomcnt',\n",
    "'calculatedfinishedsquarefeet',\n",
    "'fips',\n",
    "'latitude',\n",
    "'longitude',\n",
    "'lotsizesquarefeet',\n",
    "'regionidcity',\n",
    "'regionidcounty',\n",
    "'regionidzip',\n",
    "'yearbuilt',\n",
    "'transactiondate'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new dataframe for exploration:\n",
    "\n",
    "df = zillow_df[exploration_cols]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's rename columns for clarity\n",
    "\n",
    "df = df.rename(columns = {\n",
    "'taxvaluedollarcnt': \"home_value\",\n",
    "'bathroomcnt': \"bathroom_count\",\n",
    "'bedroomcnt': \"bedroom_count\",\n",
    "'calculatedfinishedsquarefeet': \"living_sq_feet\",\n",
    "'fips': \"fips_code\",\n",
    "'lotsizesquarefeet': \"property_sq_feet\",\n",
    "'parcelid': \"property_id\",\n",
    "'regionidcity': \"city_id\",\n",
    "'regionidcounty': \"county_id\",\n",
    "'regionidzip': \"zip_code\",\n",
    "'transactiondate': \"purchase_date\",\n",
    "'yearbuilt': \"year_built\",\n",
    "})\n",
    "df.columns.to_list() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's confirm the df shape:\n",
    "initial_shape = df.shape\n",
    "initial_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's recheck the number of null values in columns/features:\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the null percentage per feature/column:\n",
    "df.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.isnull().mean() * 100).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<u>``Notes on remaining null values:``</u>**\n",
    "\n",
    "1. since it is not know how this data was collected, it is therfore difficult to properly determine the cause of missing information (MCAR/MAR/MNAR)\n",
    "2. area specific information/values would be relatively difficult here to predict and given the relatively low percentage of null values - i dont believe at this current stage it is beneficial to handle these values in a meaningful way that would generate any additional insight into this analysis\n",
    "3. most features at this stage are comprised of <2% of null values - therefore, i will drop these and note which features contained nulls:\n",
    "\n",
    "\n",
    "|feature| null % |\n",
    "|----|----|\n",
    "|'home_value'|0.014|\n",
    "|'living_sq_feet'|0.41|\n",
    "|'property_sq_feet'|0.62|\n",
    "|'city_id'|1.88|\n",
    "|'zip_code'|0.18|\n",
    "|'year_built'|0.44|\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping nulls and rechecking df shape:\n",
    "df = df.dropna()\n",
    "shape_after_nulls = df.shape\n",
    "shape_after_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the total number of unique values/feature options per column:\n",
    "\n",
    "for col in df.columns:\n",
    "    print(f'Column/Feature:  {col}'),\n",
    "    print(f'Feature/value Type:  {df[col].dtype}'),\n",
    "    print(f'Feature min:  {df[col].min()} | Feature max:  {df[col].max()}'),\n",
    "    print(df[col].unique()),\n",
    "    print(\"-----------------------------------------------\"),\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_lst = df.columns.to_list()\n",
    "col_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the following feature to \"int\" type\n",
    "# add to function::: \n",
    "\n",
    "to_interger = [\"bedroom_count\", \"city_id\", \"county_id\", \"zip_code\"]\n",
    "df[to_interger] = df[to_interger].astype(\"int\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats:\n",
    "\n",
    "summary_stats = df.describe().T\n",
    "summary_stats[\"range\"] = summary_stats[\"max\"] - summary_stats[\"min\"]\n",
    "summary_stats.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### <u>``Univariate Feature Analysis:``</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beginning with the target variable:\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "sns.set(font_scale = .8)\n",
    "ax = sns.histplot(df[\"home_value\"])\n",
    "\n",
    "# removing axes scientific notation \n",
    "ax.ticklabel_format(style = \"plain\")\n",
    "\n",
    "plt.axvline(df[\"home_value\"].mean(), linewidth = 1, color = 'purple', alpha = 0.4, label = \"mean\")\n",
    "plt.legend()\n",
    "plt.title(\"Histogram: Home Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot where outliers are present\n",
    "plt.figure(figsize = (10, 6))\n",
    "df.boxplot(column = \"home_value\", color = \"black\")\n",
    "\n",
    "plt.semilogy()\n",
    "plt.title(\"Home Value: Outliers Identified\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mansion style homes \n",
    "# where living sq footage > 8K sq ft.\n",
    "# 90th percent quartile for homes under this criteria = ~900K\n",
    "\n",
    "over_8k = (df[df[\"living_sq_feet\"] <= 8_000].home_value)\n",
    "np.quantile(over_8k, q = [.90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of homes in the dataset that are >= 1.5MIL \n",
    "# cleaning dataset for values over 1.5mil as these homes may be considered to be \n",
    "\n",
    "print(f'Total Num. of homes that are equal to or greater than 900K in value: {(df[\"home_value\"] >= 900_000).sum()}')\n",
    "print(\"---------\")\n",
    "print(f'Total Num. of homes that have 8K or greater living sq. footage: {(df[\"living_sq_feet\"] >= 8_000).sum()}')\n",
    "print(\"---------\")\n",
    "print(f'Total Num. of homes that have eight (8) or greater bedrooms: {(df[\"bedroom_count\"] >= 8).sum()}')\n",
    "print(f'Total Num. of homes that have 6 or greater bedrooms: {(df[\"bedroom_count\"] >= 6).sum()}')\n",
    "print(f'Total Num. of homes that have 0 bedrooms: {(df[\"bedroom_count\"] == 0).sum()}')\n",
    "print(\"---------\")\n",
    "print(f'Total Num. of homes that have 6.5 or greater baths: {(df[\"bathroom_count\"] >= 6.5).sum()}')\n",
    "print(f'Total Num. of homes that have eight (8) or greater baths: {(df[\"bathroom_count\"] >= 8).sum()}')\n",
    "print(f'Total Num. of homes that have 0 baths: {(df[\"bathroom_count\"] == 0).sum()}')\n",
    "print(\"----------------------------------\")\n",
    "print(f'Percentage of homes that are equal to or greater than 900K in value: {(df[\"home_value\"] >= 900_000).mean().round(3)}')\n",
    "print(f'Percentage of homes that have 8K or greater living sq. footage: {(df[\"living_sq_feet\"] >= 8_000).mean().round(3)}')\n",
    "print(f'Percentage of homes that have greater than 6 bedrooms: {(df[\"bedroom_count\"] > 6).mean().round(3)}')\n",
    "print(f'Percentage of homes that have greater than 6.5 bathrooms: {(df[\"bathroom_count\"] > 6.5).mean().round(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping identified outlier cutoffs in the dataset \n",
    "# as these homes may represent houses larger than the majority homes, therefore making the analysis or future prediction less accurate when comparing these outliers.\n",
    "\n",
    "df = df[df[\"home_value\"] <= 900_000]\n",
    "df = df[df[\"living_sq_feet\"] <= 8_000]\n",
    "\n",
    "# also removing homes with 0 bedrooms as this appears to be slightly \"irregular\"\n",
    "df = df[(df[\"bedroom_count\"] > 0) & (df[\"bedroom_count\"] <= 6)]\n",
    "\n",
    "# also removing homes with 0 bathrooms as this appears to be slightly \"irregular\"\n",
    "df = df[(df[\"bathroom_count\"] > 0) & (df[\"bathroom_count\"] <= 6.5)]\n",
    "\n",
    "shape_after_outliers = df.shape\n",
    "shape_after_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check target again\n",
    "from acquire import plot_target, plot_continuous, plot_discrete\n",
    "\n",
    "plot_target(df[\"home_value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting and plotting continuous features/variables\n",
    "\n",
    "continuous_vars = [\n",
    " 'living_sq_feet',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'property_sq_feet',\n",
    " 'city_id',\n",
    " 'county_id',\n",
    " 'zip_code',\n",
    " 'year_built'\n",
    "]\n",
    "\n",
    "plot_continuous(df, continuous_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting categorical/potential discrete features/columns:\n",
    "\n",
    "discrete_vars = [\n",
    "    'bathroom_count',\n",
    "    'bedroom_count',\n",
    "    'fips_code'\n",
    "    ]\n",
    "\n",
    "plot_discrete(df, discrete_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting purchase date/transaction date to datetime type\n",
    "# function::::\n",
    "\n",
    "df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the purchase/transaction dates\n",
    "# creating a new column/home purchase by 2017 month\n",
    "# add function:::::\n",
    "\n",
    "# creating a lambda function to isolate the dates by month and year\n",
    "months_lst = df['purchase_date'].map(lambda dt: dt.strftime('%Y-%m'))\n",
    "\n",
    "# adding the new 2017 month series to main df as new column\n",
    "df[\"purchase_month\"] = months_lst.astype(\"str\")\n",
    "\n",
    "grouped_df = df.sort_values(\"purchase_month\").groupby('purchase_month').size().to_frame(\"count\").reset_index()\n",
    "\n",
    "sns.set(font_scale = .5, style = \"darkgrid\")\n",
    "ax = sns.countplot(x = \"purchase_month\",\n",
    "                data = df,\n",
    "                order = grouped_df[\"purchase_month\"],\n",
    "                palette = \"crest\")\n",
    "\n",
    "ax.bar_label(ax.containers[0])\n",
    "\n",
    "plt.xlabel(None)\n",
    "plt.title(\"2017 Home Purchases by Month\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**``establishing categorical/continuous variables/features``**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(f'Columns name: {col} has {df[col].nunique()} number of unique values')\n",
    "    print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting \"year_built\" to \"home_age\"\n",
    "# add function:::\n",
    "import datetime\n",
    "\n",
    "year_built = df.year_built.astype(\"int\")\n",
    "curr_year = datetime.datetime.now().year\n",
    "\n",
    "# placing column/series back into main df\n",
    "df[\"home_age\"] = curr_year - year_built\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_in_cols = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    if col in unique_in_cols:\n",
    "        unique_in_cols[col].append(df[col].nunique())\n",
    "    else:\n",
    "        unique_in_cols[col] = df[col].nunique()\n",
    "    \n",
    "unique_in_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_lst = list()\n",
    "cont_lst = list()\n",
    "\n",
    "for key, value in unique_in_cols.items():\n",
    "    if value <= 20:\n",
    "        df[key] = df[key].astype(\"object\")\n",
    "        cat_lst.append(key)\n",
    "    else:\n",
    "        cont_lst.append(key)\n",
    "\n",
    "print(cat_lst)\n",
    "print(cont_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### ``Pre-processing:``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return back to this\n",
    "# import geopandas as gpd\n",
    "# import plotly.express as px\n",
    "# import descartes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new column/feature to convert longitude and latitude to shape (location)\n",
    "# df_geo = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(\n",
    "#     df.longitude, df.latitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states = gpd.read_file(\"/Users/mijailmariano/codeup-data-science/geopandas-tutorial/data/usa-states-census-2014.shp\")\n",
    "# print(type(states))\n",
    "# states.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get built-in dataset from geopandas\n",
    "# df_geo.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a column for 1/2 bathrooms \n",
    "\n",
    "df[\"half_bathroom\"] = df[\"bathroom_count\"].astype(\"str\").str.contains(\".5\").astype(bool)\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming \"purchase_month\" values\n",
    "# add function::::\n",
    "\n",
    "year_and_month = df[\"purchase_month\"].sort_values().unique().tolist()\n",
    "month_lst = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September']\n",
    "\n",
    "df[\"purchase_month\"] = df[\"purchase_month\"].replace(\n",
    "    year_and_month,\n",
    "    month_lst)\n",
    "\n",
    "df.purchase_month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating binned categorical columns for number of bathrooms\n",
    "\n",
    "df[\"1_to_3.5_baths\"] = df[\"bathroom_count\"] <= 3.5\n",
    "df[\"4_to_6.5_baths\"] = (df[\"bathroom_count\"] > 3.5) | (df[\"bathroom_count\"] <= 6.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating binned categorical columns for number of bedrooms\n",
    "\n",
    "df[\"1_to_2_bedrooms\"] = df[\"bedroom_count\"] <= 2\n",
    "df[\"3_to_4_bedrooms\"] = (df[\"bedroom_count\"] > 2) | (df[\"bedroom_count\"] <= 4)\n",
    "df[\"5_to_6_bedrooms\"] = (df[\"bedroom_count\"] > 4) | (df[\"bedroom_count\"] <= 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also create a \"season\" column/feature where:\n",
    "# winter months = jan., feb.\n",
    "# spring months = mar., apr., may.\n",
    "# summer months = june, jul., aug.\n",
    "# fall = sep.\n",
    "\n",
    "# df[\"winter_months\"] = ((df[\"purchase_month\"] == \"January\") | (df[\"purchase_month\"] == \"February\")).astype(bool)\n",
    "# df[\"spring_months\"] = ((df[\"purchase_month\"] == \"March\") | (df[\"purchase_month\"] == \"April\") | (df[\"purchase_month\"] == \"May\")).astype(bool)\n",
    "# df[\"summer_months\"] = ((df[\"purchase_month\"] == \"June\") | (df[\"purchase_month\"] == \"July\") | (df[\"purchase_month\"] == \"August\")).astype(bool)\n",
    "# df[\"fall_months\"] = (df[\"purchase_month\"] == \"September\").astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add function:::\n",
    "\n",
    "dummy_df = pd.get_dummies(df[[\n",
    "    'fips_code', \n",
    "    'county_id',\n",
    "    'purchase_month']])\n",
    "    \n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping redundant categorical cols from main df \n",
    "# add function::::\n",
    "\n",
    "# dropping the following features/columns since they are being either 1. bucketed or 2. have created dummy variables\n",
    "df = df.drop(columns = [\n",
    "    'purchase_date',\n",
    "    'purchase_month',\n",
    "    'year_built', \n",
    "    'bedroom_count', \n",
    "    'bathroom_count', \n",
    "    'fips_code',\n",
    "    'county_id'])\n",
    "\n",
    "df = pd.concat([df, dummy_df], axis = 1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting categorical columns/features to object/string type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st split: splitting the dataset in train, validate, and test\n",
    "\n",
    "train, validate, test = train_validate_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish baselines for the train and validation subsets\n",
    "from math import sqrt\n",
    "\n",
    "baseline = round(train[\"home_value\"].mean(), 2)\n",
    "\n",
    "train['baseline'] = baseline\n",
    "validate['baseline'] = baseline\n",
    "\n",
    "train_rmse = sqrt(mean_squared_error(train.home_value, train.baseline))\n",
    "validate_rmse = sqrt(mean_squared_error(validate.home_value, validate.baseline))\n",
    "\n",
    "print('Train baseline RMSE: {:.2f}'.format(train_rmse))\n",
    "print('Validation baseline RMSE: {:.2f}'.format(validate_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping created baseline column before 2nd split\n",
    "train = train.drop(columns = \"baseline\")\n",
    "validate = validate.drop(columns = \"baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd split: splitting larger datasets into x and y variables\n",
    "\n",
    "X_train = train.drop(columns = \"home_value\")\n",
    "y_train = train['home_value']\n",
    "\n",
    "X_validate = validate.drop(columns = \"home_value\")\n",
    "y_validate = validate['home_value']\n",
    "\n",
    "X_test = test.drop(columns = \"home_value\")\n",
    "y_test = test['home_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### ``Exploration:``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting correlation amongst all data features\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "train.corr()[\"home_value\"].sort_values(ascending = False).plot.barh(figsize=(12, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### **``Hypothesis Testing:``**\n",
    "\n",
    "<u>Taget and Categorical/discrete Feature:</u> T-test\n",
    "\n",
    "\n",
    "1. 'bathroom_count'\n",
    "2. 'bedroom_count'\n",
    "3. 'fips_code'\n",
    "4. 'county_id'\n",
    "5. 'purchase_month\n",
    "\n",
    "<u>Taget and Continuous Feature:</u> Pearson R/correlation\n",
    "\n",
    "1. 'living_sq_feet'\n",
    "2. 'latitude'\n",
    "3. 'longitude'\n",
    "4. 'property_sq_feet'\n",
    "5. 'city_id'\n",
    "6. 'zip_code'\n",
    "7. 'year_built'\n",
    "8. 'home_age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.select_dtypes(exclude = [\"bool\", \"uint8\"]).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns visualizing all variables plotted against target  w/ line of best fit\n",
    "# selecting initial variables to conduct a pair plot\n",
    "\n",
    "pair_lst = X_train.select_dtypes(exclude = [\"bool\", \"uint8\"]).columns.tolist()\n",
    "\n",
    "for col in pair_lst:\n",
    "        plt.figure(figsize = (10, 4))\n",
    "        sns.set(font_scale = 1)\n",
    "\n",
    "        # plotting ea. feature against target variable with added alpha/transparency for aggregation areas\n",
    "        ax = sns.regplot(train[col], \\\n",
    "        train[\"home_value\"], \\\n",
    "        \n",
    "        # adding superficial noise to independent variables to help visualize the individual plots\n",
    "        scatter_kws = {'alpha': 1/10}, \\\n",
    "        line_kws={\n",
    "            \"color\": \"red\", 'linewidth': 1.5})\n",
    "        \n",
    "        ax.figure.set_size_inches(18.5, 8.5)\n",
    "        sns.despine()\n",
    "        # removing scientific notations\n",
    "        ax.ticklabel_format(style = \"plain\")\n",
    "        \n",
    "        # removing x_axis label\n",
    "        ax.set_xlabel(None)\n",
    "\n",
    "        plt.title(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_lst = train.select_dtypes(include = [\"bool\", \"uint8\"]).columns.tolist()\n",
    "cont_lst = train.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through categorical samples (dummy variables) to determine statistical relationship to target variable\n",
    "import scipy.stats as stats \n",
    "\n",
    "target_mean = round(y_train.mean(), 2)\n",
    "alpha = 0.05\n",
    "\n",
    "metrics = []\n",
    "for col in cat_lst:\n",
    "    sub_group = train[train[col] == 1].home_value\n",
    "    t_score, p_value = stats.ttest_1samp(sub_group, target_mean)\n",
    "\n",
    "    if p_value < alpha:\n",
    "        output = {\n",
    "            \"discrete feature\": col,\n",
    "            \"t_score\": t_score,\n",
    "            \"p_value\": p_value}\n",
    "        \n",
    "        metrics.append(output)\n",
    "\n",
    "    else:\n",
    "        print(f'Column: {col} not statistically significant.')\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "categorical_scores = pd.DataFrame(metrics)\n",
    "categorical_scores.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looping through continuous varibles to measure statistical realationship usign Pearson R method with target variable\n",
    "\n",
    "target = train[\"home_value\"]\n",
    "alpha = 0.05\n",
    "\n",
    "metrics = []\n",
    "\n",
    "for col in cont_lst:\n",
    "    r, p_value = stats.stats.pearsonr(train[col], target)\n",
    "\n",
    "    if p_value < alpha:\n",
    "        output = {\n",
    "            \"continuous feature\": col,\n",
    "            \"correlation coeffficient\": r,\n",
    "            \"p_value\": p_value}\n",
    "        \n",
    "        metrics.append(output)\n",
    "\n",
    "    else:\n",
    "        print(f'Column: {col}')\n",
    "        print(f'P_value: {round(p_value, 4)}')\n",
    "        print('Not statistically significant.')\n",
    "        print(\"--------------------------------------\")\n",
    "\n",
    "continuous_scores = pd.DataFrame(metrics)\n",
    "continuous_scores.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**``Statiscal Analysis/Conclusion Notes:``**\n",
    "\n",
    "Given the low/no statistical significance found in the following features or values, i will elect to drop or disregard the following feature options. Looking ahead, I will want to further examing/explore why these feature or options are considered \"statistically insignificant\" when determining \"home_value\".\n",
    "\n",
    "<u>``Continuous features``</u>\n",
    "1. \"city_id\"\n",
    "2. \"property_sq_feet\"\n",
    "3. \"longitude\"\n",
    "4. \"zip_code\"\n",
    "\n",
    "<u>``Categorical/discrete values:``</u>\n",
    "1. '4_to_6.5_baths'\n",
    "2. '3_to_4_bedrooms'\n",
    "3. '5_to_6_bedrooms'\n",
    "4. 'purchase_month_February'\n",
    "5. 'purchase_month_June'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establishing new datasets for feature engineering and model pre-processing\n",
    "# add function::::\n",
    "\n",
    "X_train = X_train.drop(columns = [\n",
    "    '4_to_6.5_baths',\n",
    "    '3_to_4_bedrooms',\n",
    "    '5_to_6_bedrooms',\n",
    "    'purchase_month_February', \n",
    "    'purchase_month_June',\n",
    "    \"city_id\", \n",
    "    \"property_sq_feet\",\n",
    "    \"longitude\",\n",
    "    \"zip_code\"])\n",
    "\n",
    "X_validate = X_validate.drop(columns = [\n",
    "    '4_to_6.5_baths',\n",
    "    '3_to_4_bedrooms',\n",
    "    '5_to_6_bedrooms',\n",
    "    'purchase_month_February', \n",
    "    'purchase_month_June',\n",
    "    \"city_id\", \n",
    "    \"property_sq_feet\",\n",
    "    \"longitude\",\n",
    "    \"zip_code\"])\n",
    "\n",
    "# X_test = X_test.drop(columns = [\n",
    "#     '4_to_6.5_baths',\n",
    "#     '3_to_4_bedrooms',\n",
    "#     '5_to_6_bedrooms',\n",
    "#     'purchase_month_February', \n",
    "#     'purchase_month_June',\n",
    "#     \"city_id\", \n",
    "#     \"property_sq_feet\",\n",
    "#     \"longitude\",\n",
    "#     \"zip_code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model feature cleaning function\n",
    "def clean_for_features(df):\n",
    "    df = df.drop(columns = [\n",
    "        '4_to_6.5_baths',\n",
    "        '3_to_4_bedrooms',\n",
    "        '5_to_6_bedrooms',\n",
    "        'purchase_month_February', \n",
    "        'purchase_month_June',\n",
    "        \"city_id\", \n",
    "        \"property_sq_feet\",\n",
    "        \"longitude\",\n",
    "        \"zip_code\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**``Feature engineering for MVP Variables/features:``**\n",
    "\n",
    "which x variables do i believe will hold the most significance in modeling?\n",
    "\n",
    "* purchase month\n",
    "* living sq feet\n",
    "* home age\n",
    "* number of bathrooms/bedrooms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>``Scaling continuous training (independent) variables/features:``</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Scaler Plots\n",
    "\n",
    "cont_lst = X_train.select_dtypes(exclude = [\"object\", \"uint8\", \"bool\"]).columns.tolist()\n",
    "\n",
    "for col in cont_lst:\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(X_train[[col]])\n",
    "\n",
    "    x_scaled = scaler.transform(X_train[[col]])\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(121)\n",
    "    ax = sns.histplot(X_train[[col]], bins = 25, edgecolor = 'black', label = col)\n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Original: {col}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax = sns.histplot(x_scaled, bins=25, edgecolor = 'black', label = \"scaled\")\n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Scaled: {col}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Scaler\n",
    "\n",
    "for col in cont_lst:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train[[col]])\n",
    "\n",
    "    x_scaled = scaler.transform(X_train[[col]])\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(121)\n",
    "    ax = sns.histplot(X_train[[col]], bins = 25, edgecolor = 'black', label = col)\n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Original: {col}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax = sns.histplot(x_scaled, bins=25, edgecolor = 'black', label = \"scaled\")\n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Scaled: {col}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Sccaler Plots\n",
    "\n",
    "for col in cont_lst:\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train[[col]])\n",
    "\n",
    "    x_scaled = scaler.transform(X_train[[col]])\n",
    "\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(121)\n",
    "    ax = sns.histplot(X_train[[col]], bins = 25, edgecolor = 'black', label = col)\n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\") \n",
    "    plt.title(f'Original: {col}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    ax = sns.histplot(x_scaled, bins=25, edgecolor = 'black', label = \"scaled\")\n",
    "    # removing axes scientific notation \n",
    "    ax.ticklabel_format(style = \"plain\")\n",
    "    plt.title(f'Scaled: {col}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling continuous features/data with sklearn \"MinMaxScaler\"\n",
    "# creating function to do this\n",
    "# add function::::\n",
    "\n",
    "def scaled_data(df, scaled_cols):\n",
    "    # creating a copy of the original zillow/dataframe\n",
    "    df_scaled = df.copy()\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    scaler.fit(df_scaled[scaled_cols])\n",
    "\n",
    "    df_scaled[scaled_cols] = scaler.transform(df_scaled[scaled_cols])\n",
    "\n",
    "    # returning newly created dataframe with scaled data\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creaing a new list for scaling (excludes target variable)\n",
    "scaled_lst = X_train.columns.tolist()\n",
    "scaled_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating new X_train, and X_validate datasets w/scaled data\n",
    "X_train = scaled_data(X_train, scaled_lst)\n",
    "X_validate = scaled_data(X_validate, scaled_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing kbest function on train dataset:\n",
    "\n",
    "select_kbest(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing recursive function on train dataset:\n",
    "\n",
    "recursive_feature_eng(X_train, y_train, 10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# initiate, and fit\n",
    "rfecv = RFECV(\n",
    "    estimator = LinearRegression(),\n",
    "    min_features_to_select = 5\n",
    ")\n",
    "\n",
    "rfecv = rfecv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_lst = X_train.columns[rfecv.support_].tolist()\n",
    "feature_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr_model = lr.fit(X_train, y_train)\n",
    "\n",
    "lars = LassoLars()\n",
    "lars_model = lars.fit(X_train, y_train)\n",
    "\n",
    "glm = TweedieRegressor(alpha = 1, power = 0)\n",
    "glm_model = glm.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training R-squared:\", lr_model.score(X_train, y_train))\n",
    "print(\"Training R-squared:\", lars_model.score(X_train, y_train))\n",
    "print(\"Training R-squared:\", glm_model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a looping statement to evaluate all models\n",
    "from math import sqrt\n",
    "\n",
    "models = [lr_model, lars_model, glm_model]\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    train_model = model.predict(X_train)\n",
    "    rmse_train = sqrt(mean_squared_error(y_train,\n",
    "                                         train_model))\n",
    "    \n",
    "    validate_model = model.predict(X_validate)\n",
    "    rmse_validate = sqrt(mean_squared_error(y_validate,\n",
    "                                         validate_model))\n",
    "    \n",
    "    print('RMSE for {} model on the train dataset: {}.'.format(model, round(rmse_train, 2)))\n",
    "    print('RMSE for {} model on the validate dataset: {}.'.format(model, round(rmse_validate, 2)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "#### <u>**``Baseline Prediction & Evaluations:``**</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the independent and dependent variables\n",
    "X_var = pd.DataFrame(X_validate[feature_lst])\n",
    "y_var = pd.DataFrame(y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat([X_var, y_var], axis = 1)\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a mean baseline prediction\n",
    "baseline_mean_predictions = round(y_validate.mean(), 2)\n",
    "baseline_mean_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding baseline to model_df\n",
    "model_df[\"baseline_mean_predictions\"] = baseline_mean_predictions\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating validate model predictions and assigning to dataframe\n",
    "lr_predictions = lr_model.predict(X_validate)\n",
    "model_df[\"linear_predictions\"] = lr_predictions.round(2)\n",
    "\n",
    "lars_predictions = lars_model.predict(X_validate)\n",
    "model_df[\"lars_predictions\"] = lars_predictions.round(2)\n",
    "\n",
    "glm_predictions = lars_model.predict(X_validate)\n",
    "model_df[\"glm_predictions\"] = glm_predictions.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the df\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_errors(y, y_hat):\n",
    "    # importing math.sqrt module for calculations\n",
    "    from math import sqrt\n",
    "    \n",
    "    # generating model residuals and residuals squared\n",
    "    df = y - y_hat\n",
    "    df[\"residual^2\"] = df.round(2) ** 2\n",
    "\n",
    "    # generating sum of squared error\n",
    "    SSE = sum(df[\"residual^2\"])\n",
    "\n",
    "    # generating explained sum of squares\n",
    "    ESS = sum((y_hat - y.mean()) ** 2)\n",
    "\n",
    "    # generating total sum of squares error\n",
    "    TSS = ESS + SSE\n",
    "\n",
    "    # generating mean squared error\n",
    "    MSE = SSE/len(y)\n",
    "\n",
    "    # generating root mean squared error\n",
    "    RMSE = sqrt(MSE)\n",
    "\n",
    "    return SSE, ESS, TSS, MSE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting error reports for baseline predictions\n",
    "\n",
    "SSE, ESS, TSS, MSE, RMSE = regression_errors(model_df[\"home_value\"], model_df[\"baseline_mean_predictions\"])\n",
    "\n",
    "print('Baseline SSE = {:.1f}'.format(SSE))\n",
    "print('Baseline ESS = {:.1f}'.format(ESS))\n",
    "print('Baseline TSS = {:.1f}'.format(TSS))\n",
    "print('Baseline MSE = {:.1f}'.format(MSE))\n",
    "print('Baseline RMSE = {:.1f}'.format(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting error reports for linear model predictions\n",
    "\n",
    "SSE, ESS, TSS, MSE, RMSE = regression_errors(model_df[\"home_value\"], model_df[\"linear_predictions\"])\n",
    "\n",
    "print('Model SSE = {:.1f}'.format(SSE))\n",
    "print('Model ESS = {:.1f}'.format(ESS))\n",
    "print('Model TSS = {:.1f}'.format(TSS))\n",
    "print('Model MSE = {:.1f}'.format(MSE))\n",
    "print('Model RMSE = {:.1f}'.format(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_report(y, y_hat):\n",
    "    # importing math.sqrt module for calculations\n",
    "    from math import sqrt\n",
    "    \n",
    "    # generating model residuals and residuals squared\n",
    "    df = y - y_hat\n",
    "    df[\"residual^2\"] = df.round(2) ** 2\n",
    "\n",
    "    # generating sum of squared error\n",
    "    SSE = sum(df[\"residual^2\"])\n",
    "\n",
    "    # generating explained sum of squares\n",
    "    ESS = sum((y_hat - y.mean()) ** 2)\n",
    "\n",
    "    # generating total sum of squares error\n",
    "    TSS = ESS + SSE\n",
    "\n",
    "    # generating mean squared error\n",
    "    MSE = SSE/len(y)\n",
    "\n",
    "    # generating root mean squared error\n",
    "    RMSE = sqrt(MSE)\n",
    "\n",
    "    print(f'{y_hat.name} SSE: {SSE}')\n",
    "    print(f'{y_hat.name} ESS: {ESS}')\n",
    "    print(f'{y_hat.name} TSS: {TSS}')\n",
    "    print(f'{y_hat.name} MSE: {MSE}')\n",
    "    print(f'{y_hat.name} RMSE: {RMSE}')\n",
    "\n",
    "    return SSE, ESS, TSS, MSE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SSE, ESS, TSS, MSE, RMSE = get_error_report(model_df[\"home_value\"], model_df[\"glm_predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final error report df\n",
    "\n",
    "final_report = {\n",
    "'Models': ['Baseline Predictions', 'Linear Regression Predictions', 'Tweedie Regressor Predictions'],\n",
    "'Explained Sum of Squares (ESS)': [0.3, 121233107350265.9, 114322513495073.8],\n",
    "'Mean Sum Error (MSE)': [41914677866.4, 32010289438.0, 32057696544.18],\n",
    "'Root Mean Squared Error (RMSE)': [204730.7, 178914.2, 179046.63]\n",
    "}\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "final_report = pd.DataFrame(final_report)\n",
    "final_report.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting actual home values, baseline_mean_predictions predictions, and model predictions\n",
    "# add function:::\n",
    "\n",
    "plt.figure(figsize = (16, 10))\n",
    "\n",
    "# plotting baseline prediction line of best fit\n",
    "plt.plot(model_df['home_value'].sample(300, \n",
    "        random_state = 123), \n",
    "        model_df['baseline_mean_predictions'].sample(300, random_state = 123), \n",
    "        alpha=0.5,\n",
    "        color='red', \n",
    "        ls = ':', \n",
    "        label='_nolegend_')\n",
    "\n",
    "# plotting home value line of best fit\n",
    "plt.plot(model_df['home_value'].sample(300, random_state = 123), \n",
    "        model_df['home_value'].sample(300, random_state = 123), \n",
    "        alpha=0.5,\n",
    "        color='purple', \n",
    "        label='_nolegend_')\n",
    "\n",
    "# linear model plot\n",
    "plt.scatter(model_df['home_value'].sample(300, random_state = 123), \n",
    "            model_df['linear_predictions'].sample(300, random_state = 123), \n",
    "            alpha=0.5,\n",
    "            color='red', \n",
    "            s=100,\n",
    "            label='Linear Regression')\n",
    "\n",
    "# lasso lars plot\n",
    "plt.scatter(model_df['home_value'].sample(300, random_state = 123), \n",
    "            model_df['lars_predictions'].sample(300, random_state = 123), \n",
    "            alpha=0.5,\n",
    "            color='yellow', \n",
    "            s=100, \n",
    "            label='Lasso Lars')\n",
    "\n",
    "# tweedie/glm plot\n",
    "plt.scatter(model_df['home_value'].sample(300, random_state = 123), \n",
    "            model_df['glm_predictions'].sample(300, random_state = 123), \n",
    "            alpha=0.5,\n",
    "            color='green', \n",
    "            s=100, \n",
    "            label='Tweedie Regressor')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Home Value\")\n",
    "plt.ylabel('Predicted Home Value')\n",
    "plt.title('Actual Home Values vs Model Predicted Home Values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Residual (error) Plot\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.axhline(label='No Error', \n",
    "            color = 'purple',\n",
    "            ls = ':')\n",
    "\n",
    "# plotting linear model\n",
    "plt.scatter(model_df['home_value'].sample(300, random_state = 123), \n",
    "            model_df['linear_predictions'].sample(300, random_state = 123) - model_df['home_value'].sample(300, random_state = 123), \n",
    "            alpha=0.5,\n",
    "            color='red', \n",
    "            s=100, \n",
    "            label='Linear Regression')\n",
    "\n",
    "# plotting lasso lars model\n",
    "plt.scatter(model_df['home_value'].sample(300, random_state = 123), \n",
    "            model_df['lars_predictions'].sample(300, random_state = 123) - model_df['home_value'].sample(300, random_state = 123), \n",
    "            alpha=0.5,\n",
    "            color='yellow', \n",
    "            s=100, \n",
    "            label='Lasso Lars')\n",
    "\n",
    "# plotting tweedie model\n",
    "plt.scatter(model_df['home_value'].sample(300, random_state = 123), \n",
    "            model_df['glm_predictions'].sample(300, random_state = 123) - model_df['home_value'].sample(300, random_state = 123), \n",
    "            alpha=0.5,\n",
    "            color='green', \n",
    "            s=100, \n",
    "            label='Tweedie Regressor')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Actual Home Value')\n",
    "plt.ylabel('Residual Error')\n",
    "plt.title('Model Residual Plot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of my model predictions (linear & tweedie)\n",
    "# add function:::\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "sns.set(style = \"darkgrid\")\n",
    "\n",
    "plt.hist(model_df['home_value'], color='blue', alpha=0.4, label='Actual Home Value')\n",
    "plt.hist(model_df['linear_predictions'], color='red', alpha=0.4, label='Linear Regression')\n",
    "plt.hist(model_df['lars_predictions'], color='green', alpha=0.4, label='Laso Lars')\n",
    "\n",
    "\n",
    "plt.xlabel('Home Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Home Values by Predictive Model')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model predictions and score on Test Dataset\n",
    "\n",
    "test_df = pd.DataFrame(y_test)\n",
    "print(test_df.shape )\n",
    "test_df.head() # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling X_test data using created scaling function\n",
    "\n",
    "X_test = scaled_data(X_test, scaled_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning X_test dataset for model features\n",
    "\n",
    "X_test = clean_for_features(X_test)\n",
    "X_test.shape # checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"model_predictions\"] = lr_model.predict(X_test)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rmse_test = sqrt(mean_squared_error(test_df['home_value'], test_df['model_predictions']))\n",
    "print(f'RMSE for OLS model on the test dataset: {round(rmse_test, 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse = pd.DataFrame({\n",
    "    \"Test\": [\"Baseline\", \"Train\", \"Validate\", \"Final\"],\n",
    "    \"RMSE\": [204730.70,178296.33,178914.20,177111.14],\n",
    "    \"Relative Diff.\": [0, .15, 0, .01]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
